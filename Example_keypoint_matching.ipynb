{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiO97ZRkp5Gn6nI8UMxvPc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyscan6003/etivities/blob/main/Example_keypoint_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izizIlr9gKxk"
      },
      "source": [
        "# Feature Matching with SIFT Keypoint Descriptor\n",
        "\n",
        "In this notebook we will use the OpenCV Implementation of the SIFT Detector and Keypoint Descriptor to demonstrate Keypoint Matching with OpenCV functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpkPmVET5Vdz"
      },
      "source": [
        "We must install a previous version of OpenCV in order to access the SIFT algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Cw9twlu5lWL"
      },
      "source": [
        "**Housekeeping**: Import Libraries (including previous version of opencv in order to access the SIFT algorithm), define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxJN1aPGgKNE"
      },
      "source": [
        "! yes | pip3 uninstall opencv-python\n",
        "\n",
        "! yes | pip3 uninstall opencv-contrib-python\n",
        "\n",
        "! yes | pip3 install opencv-python==3.4.2.17\n",
        "\n",
        "! yes | pip3 install opencv-contrib-python==3.4.2.17"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b71mWQYGvQxz"
      },
      "source": [
        "import numpy as np\n",
        "import urllib.request\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from mpl_toolkits import mplot3d\n",
        "from scipy import signal\n",
        "\n",
        "def url_to_image(url):\n",
        "\tresp = urllib.request.urlopen(url)\n",
        "\ttemp_image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "\ttemp_image = cv2.imdecode(temp_image, cv2.IMREAD_COLOR)\n",
        "\ttemp_image = cv2.cvtColor(temp_image, cv2.COLOR_BGR2RGB) # OpenCV defaults to BGR, but we need RGB here..\n",
        "\treturn temp_image\n",
        "\n",
        "#Function to\n",
        "def read_images():\n",
        "    image = url_to_image(image_url)\n",
        "    x,y,z = np.shape(image)\n",
        "    # Take Centre Crop (zoomed image)\n",
        "    x_trg = int(x*scale_factor)\n",
        "    y_try = int(y*scale_factor)\n",
        "    bdx = int((x-x_trg)//2)\n",
        "    bdy = int((y-x_trg)//2)\n",
        "    image_crop = image[bdx:x-bdx,bdy:y-bdy]\n",
        "    # Resize original iamge to match cropsize.\n",
        "    x,y,z = np.shape(image_crop)\n",
        "    image_scale = cv2.resize(image, dsize=(y, x), interpolation=cv2.INTER_CUBIC)\n",
        "    return image_scale,image_crop\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9S1Ut7vR5pM"
      },
      "source": [
        "#Read Image:\n",
        "Read Image from URL and produced two scaled versions for keypoint matching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnfoNRyavYr5"
      },
      "source": [
        "# Image ULR (This can be changed to any image)\n",
        "#image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Monarch_butterfly_in_BBG_%2884685%29.jpg/1280px-Monarch_butterfly_in_BBG_%2884685%29.jpg\"\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Bologna_Statue_of_Galvani.jpg/1280px-Bologna_Statue_of_Galvani.jpg\"\n",
        "\n",
        "#Read in images\n",
        "scale_factor = 0.85\n",
        "ref_image,test_image = read_images()\n",
        "\n",
        "# Plot reference and test images\n",
        "f, axarr = plt.subplots(1,2,figsize=(10,20))\n",
        "axarr[0].imshow(ref_image)\n",
        "axarr[0].axis('off')\n",
        "axarr[0].title.set_text('Reference Image')\n",
        "axarr[1].imshow(test_image)\n",
        "axarr[1].axis('off')\n",
        "axarr[1].title.set_text('Test Image')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWbsUYeVAxBi"
      },
      "source": [
        "#SIFT Descriptor\n",
        "\n",
        "Instantiate the OpenCV implementation of the SIFT detector/descriptor\n",
        "\n",
        "This performed using `cv2.xfeatures2d.SIFT_create(nfeatures=50)` where the number of features to return can be adjusted. (lower numbers are perferred for this demo so that correspondances between keypoints can be seen in the images)\n",
        "\n",
        "The keypoint locations and descriptor vectors are returned using sift.`detectAndCompute(img1,None)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qNsuoQuveHm"
      },
      "source": [
        "# Convert images to grey scale\n",
        "img1 = cv2.cvtColor(ref_image, cv2.COLOR_BGR2GRAY)\n",
        "img2 = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Initiate SIFT\n",
        "sift = cv2.xfeatures2d.SIFT_create(nfeatures=50)\n",
        "# find the keypoints and descriptors with ORB\n",
        "kp1, des1 = sift.detectAndCompute(img1,None)\n",
        "kp2, des2 = sift.detectAndCompute(img2,None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IigHBMcUCKvU"
      },
      "source": [
        "The location of the keypoints on the reference and test images are plotted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq45ZzhYvehZ"
      },
      "source": [
        "keypoints_without_size = np.copy(ref_image)\n",
        "testkp_without_size = np.copy(test_image)\n",
        "keypoints_with_size = np.copy(ref_image)\n",
        "\n",
        "cv2.drawKeypoints(ref_image, kp1, keypoints_without_size, color = (0, 255, 0))\n",
        "cv2.drawKeypoints(test_image, kp2, testkp_without_size, color = (0, 255, 0))\n",
        "cv2.drawKeypoints(ref_image, kp1, keypoints_with_size, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "\n",
        "# Display image with and without keypoints size\n",
        "fx, plots = plt.subplots(1, 3, figsize=(20,10))\n",
        "\n",
        "plots[0].set_title(\"Reference Image keypoints With Scale\")\n",
        "plots[0].imshow(keypoints_with_size, cmap='gray')\n",
        "\n",
        "plots[1].set_title(\"Reference Image keypoints Location Only\")\n",
        "plots[1].imshow(keypoints_without_size, cmap='gray')\n",
        "\n",
        "plots[2].set_title(\"Test Image keypoints Location Only\")\n",
        "plots[2].imshow(testkp_without_size, cmap='gray')\n",
        "\n",
        "# Print the number of keypoints detected in the training image\n",
        "print(\"Number of Keypoints Detected In The Training Image: \", len(kp1))\n",
        "\n",
        "# Print the number of keypoints detected in the query image\n",
        "print(\"Number of Keypoints Detected In The Query Image: \", len(kp2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40sc3hlRCdD-"
      },
      "source": [
        "Keypoint Matching between the descriptors is performed using the openCV brute force matcher (cv2.BFMatcher), this uses k-nearest neighbours algorithm to find correspondences between keypoints from the different images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-tJoiNpxLi0"
      },
      "source": [
        "bf = cv2.BFMatcher()\n",
        "matches = bf.knnMatch(des1,des2,k=2)\n",
        "\n",
        "# Apply ratio test\n",
        "good = []\n",
        "for m,n in matches:\n",
        "    if m.distance < 0.75*n.distance:\n",
        "        good.append([m])\n",
        "# cv.drawMatchesKnn expects list of lists as matches.\n",
        "img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(img3),plt.show()\n",
        "#See the result below:"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}