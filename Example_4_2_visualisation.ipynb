{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example_4_2_visualisation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyscan6003/etivities/blob/main/Example_4_2_visualisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3-mOwMBY3Jp"
      },
      "source": [
        "# Visulisation Example\n",
        "\n",
        "This Tensorflow 2.x implementation is based on the paper by [Manhendran (2015)](https://arxiv.org/pdf/1512.02017.pdf)  \n",
        "\n",
        "The layers of the pre-trained VGG-16 network are to be visualised using the activation maximisation technique presented in the paper. \n",
        "\n",
        "It is important to note each Feature maps of the network has a different size receptive field. We set the size of the input image to be slightly larger than the receptive field in each case. (The receptive field must be known or calculated for each feature map visualised)\n",
        "\n",
        "![link text](https://github.com/tonyscan6003/etivities/blob/main/vis_act_max.jpg?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUM0kzllcTuP"
      },
      "source": [
        "##Housekeeping\n",
        "Load Packages & Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHQqEZXHwFGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6940523f-3363-4438-f724-5bf5f9d67fb7"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcfCwizMq133"
      },
      "source": [
        "# Helper function to output images at end/during optimisation\n",
        "def display_image(image):\n",
        "   \n",
        "    title = ['Generated Image']\n",
        "    plt.figure(figsize=(3,3))\n",
        "    plt.title(title)\n",
        "    plt.imshow(np.clip(image[0,:,:,:], 0, 255).astype(\"uint8\"),aspect=\"auto\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "def display_array(img_array,array_size):\n",
        "    #unprocess model input and output\n",
        "    #p_low_res = unprocess_image(low_res_input)\n",
        "    #up_model_op = unprocess_image(model_op)\n",
        "    n = array_size[0]\n",
        "    m = array_size[1]\n",
        "    plt.figure(figsize=(3*m,3*n))\n",
        "    plt.title('Imaged Filters') \n",
        "    ptr = 1\n",
        "    for i in range(n):      \n",
        "       for j in range(m):\n",
        "          ax=plt.subplot(n,m,ptr)     \n",
        "          plt.imshow(np.clip((img_array[ptr,:,:,:]+127.5), 0, 255).astype(\"uint8\"),aspect=\"auto\")  \n",
        "          ptr = ptr+1\n",
        "          plt.axis('off')\n",
        "    plt.show()       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzcOGRZ1e7m7"
      },
      "source": [
        "# Imagenet Rgb\n",
        "imagenet_rgb_values = [123.68, 116.779, 103.939] # Preprocessing values for imagnet (no need to adjust)\n",
        "\n",
        "def process_image(input_img):\n",
        "    op_img =[]\n",
        "    r = input_img[:,:, :, 0] - imagenet_rgb_values[0]\n",
        "    g = input_img[:,:, :, 1] - imagenet_rgb_values[1]\n",
        "    b = input_img[:,:, :, 2] - imagenet_rgb_values[2]\n",
        "    op_img = tf.stack([b,g,r],axis = 3)\n",
        "    return op_img\n",
        "\n",
        "def img_clip(image):\n",
        "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1_sLQZMwKEl"
      },
      "source": [
        "## Define Target Layer & Hyper parameters.\n",
        "Parameters have same definition as in Manhedran paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeDUYC5ndNty"
      },
      "source": [
        "\n",
        "# Choose target Layer of network (becomes output of reduced model)\n",
        "output_layer = ['block4_conv2'] \n",
        "rho = 76 #receptive field size for layer\n",
        "n_c = 5  # centre neuron (choose 1/2 Feature map spatial dimension)\n",
        "filter_list =[10,51,123,198,234,343,411,455,501] # Which Feature maps to visualise\n",
        "\n",
        "# Random image is marginaly larger than receptive field\n",
        "H=100    #Input random noise image H\n",
        "W=100    #Input random noise image H\n",
        "T= 4     # Jitter value\n",
        "\n",
        "# Hyperprameters.\n",
        "B =  80\n",
        "V = B/6.5\n",
        "alpha_val = 6\n",
        "beta_val = 2\n",
        "M = 5500.0       # Normalisation parater experimentally determined\n",
        "Z = M*(rho**2)\n",
        "C=30000          # The C parameter balances the loss (activation) against regularisation it will vary depending on the layer\n",
        "jitter_on=True\n",
        "\n",
        "# Scalar for bounded range.\n",
        "norm_r_alpha = 1/(H*W*(B**alpha_val))  # Alplha regulariser\n",
        "# Scale for total variation regulation.\n",
        "norm_r_beta  = 1/(H*W*(V**beta_val))  # Beta regulariser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRx-skwCtuVh"
      },
      "source": [
        "## Import Model\n",
        "In the following Cell the Vgg16 keras model is loaded. A modified model \"red_model\" is created that has a reduced input size to match that of the random image (receptive filed target neuron).Thes layers after the Feature map of interest are omitted from the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUa5_Md_edix"
      },
      "source": [
        "vgg16_model = tf.keras.applications.VGG16(weights=\"imagenet\", include_top=False,input_tensor=Input(shape=(H, W, 3)))\n",
        "\n",
        "\n",
        "# Function to get layer output tensors & layer shapes.\n",
        "def layer_data(layer_names):\n",
        "    layer_shape = []  \n",
        "    model_ops = []\n",
        "    for layer in layer_names:\n",
        "        model_ops.append(vgg16_model.get_layer(layer).output)\n",
        "        layer_shape.append(vgg16_model.get_layer(layer).output_shape)\n",
        "    return model_ops,layer_shape  \n",
        "\n",
        "# Define reduced model\n",
        "model_ops,output_layer_shape = layer_data(output_layer) \n",
        "red_model = tf.keras.Model(inputs = vgg16_model.input, outputs=model_ops ) \n",
        "print(output_layer_shape)\n",
        "\n",
        "red_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ClfFjjyro1O"
      },
      "source": [
        "## Define Loss & Regularisation functions: \n",
        "\n",
        "In the activiation maximisation method the goal is to find the input image $x$ that maximise the value of a particular neuron. Rather than using gradient ascent, we can use gradient descent and minimise the negative of the value of the neuron. The loss function is written as the dot product of the Feature maps $\\Phi(x)$ that represent the image and an indicator variable $\\Phi_0$.\n",
        "\n",
        " $l = \\frac{\\langle\\Phi(x),\\Phi_0\\rangle}{Z}$\n",
        "\n",
        "As can be seen in the function `main_loss` the activation maximisation loss is simply written as a reduce sum of the feature map of interest. (This will often be a single neuron, an indicator varaible is not required as we explicitly select the Feature map or neuron of interest) \n",
        "\n",
        "Using activation maximisation loss on it's own will often not produce visually appealing images, so two regularisation functions are included to ensure the trained input images look natural. \n",
        "\n",
        "The bounded variation uses a soft constraint to penalise large pixel values in the image $x$ (Note the additional hard constraint mentioned in the paper is not implemented)\n",
        "\n",
        "${N_\\alpha}(x)=\\frac{1}{HWB^\\alpha}\\sum_{v=1}^H\\sum_{u=1}^W(\\sum_{k=1}^Dx(v,u,k)^2)^\\frac{\\alpha}{2}$\n",
        "\n",
        "This regualarisation equation is simply implemented as the function `bounded_rng_reg` where the sum of the square of the pixel of the input image are obtained and additionally raised to the power of $\\frac{\\alpha}{2}$\n",
        "\n",
        "The second regularisation term is Total variation, as discussed in the paper this regulariser helps to promote consistent patches and avoid spikes in the image. (More detail on the [Total Variation](https://hal.archives-ouvertes.fr/hal-01309685v3/document) can be found here)\n",
        "\n",
        "${R_{TV^\\beta}}(x)=\\frac{1}{HWB^\\beta}\\sum_{uvk}\\Bigl((x(v,u+1,k)-x(v,u,k))^2+(x(v+1,u,k)-x(v,u,k))^2\\Bigr)^\\frac{\\beta}{2}$\n",
        "\n",
        "This regularisation term is implemented in the function `TV_loss`\n",
        "\n",
        "A third type of regularisation is Jitter, this promotes sharper images. This is not implemented as part of the loss function but instead a random crop of the input image is taken each time the image is applied to the network during training (See `train_step`) function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCwlx7mjevXG"
      },
      "source": [
        "# Loss function due to activation maximisation.\n",
        "def main_loss(output_features):\n",
        "    #loss = tf.keras.backend.mean(model_output[:, :, :, filter_index])\n",
        "    loss = -(1/Z)*tf.math.reduce_sum(output_features[:, n_c, n_c, filter_index])\n",
        "    return C*loss\n",
        "    \n",
        "    \n",
        "# Add bouded range regulariser.\n",
        "def bounded_rng_reg(img):\n",
        "    #square all values\n",
        "    x = tf.math.square(img)\n",
        "    #sum over channels\n",
        "    x = tf.keras.backend.sum(x,axis=3)\n",
        "    #raise to a power\n",
        "    x = tf.keras.backend.pow(x,alpha_val/2)\n",
        "    #sum over remaining channels\n",
        "    x=tf.math.reduce_sum(x)\n",
        "    #apply scalar function.\n",
        "    x = norm_r_alpha*x\n",
        "    return x\n",
        "  \n",
        "  \n",
        "# conditional value to bound the output  \n",
        "#  Return on a per element basis, so that are gradients for each image value.\n",
        "def cond_value(img):\n",
        "    x = tf.math.square(img)\n",
        "    x = tf.keras.backend.sum(x,axis=3)\n",
        "    y = tf.keras.backend.pow(x,1/2)  \n",
        "    return y\n",
        "  \n",
        "  \n",
        "# Differences total variation loss\n",
        "def high_pass_x_y(image):\n",
        "  x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n",
        "  y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n",
        "  # Pad zeros along difference dimension for von neumann boundary. (dim reduced)\n",
        "  pad_x = [[0,0],[0, 0], [0, 1],[0,0]]\n",
        "  pad_y = [[0,0],[0, 1], [0, 0],[0,0]]      \n",
        "  x_var = tf.pad(x_var, pad_x, 'CONSTANT', constant_values=0)\n",
        "  y_var = tf.pad(y_var, pad_y, 'CONSTANT', constant_values=0)       \n",
        "  return x_var, y_var\n",
        "\n",
        "\n",
        "# Total variation loss\n",
        "def TV_loss(image):\n",
        "  x_deltas, y_deltas = high_pass_x_y(image)\n",
        "  x = (x_deltas**2) + (y_deltas**2)\n",
        "  x=(tf.keras.backend.pow(x,beta_val/2))\n",
        "  x=tf.reduce_sum(x)\n",
        "  tv_loss = norm_r_beta*x\n",
        "  return tv_loss\n",
        "\n",
        "def total_loss(output_features,img):\n",
        "    l1 =  main_loss(output_features)\n",
        "    # replace conditional with log barrier to make continuous for differentiation\n",
        "    l1_bnd =   bounded_rng_reg(img)\n",
        "    tv_loss = TV_loss(img)\n",
        "    t_loss =  l1+l1_bnd+tv_loss\n",
        "    return l1,l1_bnd,t_loss,tv_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad5KpKTiBLUF"
      },
      "source": [
        "## Training step\n",
        "\n",
        "The paper defines a stocasitic gradient descent algorithm, we can use the built in SGD optimiser to produce the image for the required filters.\n",
        "\n",
        "In order to use the activation maximisation technique we cannot use the standard Keras model.fit methodology. Instead we directly apply the Tensorflow automatic differentiation using [gradient tape](https://www.tensorflow.org/guide/autodiff)\n",
        "\n",
        "The keras implementation adam optimiser produces good results and is used instead of the SGD algorithm detailed in the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEmPoxdke263"
      },
      "source": [
        "\n",
        "# optimiser for gradients\n",
        "adam_optimiser = tf.keras.optimizers.Adam(0.1, beta_1=0.09, beta_2=0.99, epsilon=1e-7)\n",
        "\n",
        "# Traing step function\n",
        "def train_step(rnd_img_data):\n",
        "  with tf.GradientTape() as gen_tape:\n",
        "    \n",
        "    gen_tape.watch(rnd_img_data)  \n",
        "    if jitter_on == True:   \n",
        "       inp_img = tf.image.random_crop(rnd_img_data,(1,H,W,3))\n",
        "    else:\n",
        "        inp_img = rnd_img_data\n",
        "    op_features=red_model(process_image(inp_img+127.0))\n",
        "    nx = np.max(cond_value(rnd_img_data))\n",
        "    l1,l1_bnd,t_loss,tv_loss = total_loss(op_features,rnd_img_data)\n",
        " \n",
        "  img_grads = gen_tape.gradient(t_loss,rnd_img_data)\n",
        "  adam_optimiser.apply_gradients([(img_grads,rnd_img_data)])\n",
        "  return l1,l1_bnd,t_loss,rnd_img_data,nx,tv_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUuvL6MATyXK"
      },
      "source": [
        "## Run Training\n",
        "Running the training step. The number of iterations of training can be set. An image corresponding to each layer of the feature map is plotted as it is completed. (The exact set of feature maps is determined by the variable `filter_list` defined earlier.) (The max number that can be plotted depends on the depth of the feature map, note some deep feature maps have a depth of 512!!!)\n",
        "\n",
        "NB: On occasion no image is produced, this is due to the stochastic nature of the process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1-yPqHCr9d_"
      },
      "source": [
        "iterations = 2000\n",
        "\n",
        "layer_images =  tf.Variable(np.zeros([1,H+T,W+T,3]))\n",
        "layer_images = tf.cast(layer_images, dtype='float32')\n",
        "\n",
        "# Code to produce multiple filter images in each layer.\n",
        "for filter_index in filter_list:\n",
        "    # reset random vector\n",
        "    rnd_img_data = tf.Variable( tf.random.normal([1, H+T,W+T,3],mean=0,stddev=30.0,name=\"input_img_data\"))\n",
        "    rnd_img_data = tf.cast(rnd_img_data, dtype='float32')\n",
        "    \n",
        "    for iterations in range(iterations):\n",
        "        l1,l1_bnd,t_loss,rnd_img_data,nx,tv_loss = train_step(rnd_img_data)\n",
        "        if iterations % 100 == 0:\n",
        "          print('Iteration Number =',iterations)\n",
        "    layer_images = tf.concat([layer_images, rnd_img_data],0) \n",
        "    display_image(rnd_img_data+127.0)\n",
        "# reshape into grid and display (Set depending on no. feature maps visualised).    \n",
        "display_array(layer_images,(2,3))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NFi0nQHWsYU"
      },
      "source": [
        "# Run training step (Debug mode)\n",
        "\n",
        "In the cell below the training step is run for a single feature map. Images are output more frequently and values of the loss and regulariser are output to verify that the training is processing correctly. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtjlRw7KX4Qs"
      },
      "source": [
        "# Test one image\n",
        "\n",
        "iterations = 2000\n",
        "filter_index =245\n",
        "\n",
        "\n",
        "# initialise noise vector\n",
        "rnd_img_data = tf.Variable( tf.random.normal([1, H+T,W+T,3],mean=0,stddev=30.0,name=\"input_img_data\"))\n",
        "rnd_img_data = tf.cast(rnd_img_data, dtype='float32')\n",
        "\n",
        "for iterations in range(iterations):\n",
        "    #pert_noise = tf.random.normal([1, H+T,W+T,3],mean=0.0,stddev=1)\n",
        "    #latent_vector=rnd_img_data+pert_noise\n",
        "    l1,l1_bnd,t_loss,rnd_img_data,nx,tv_loss = train_step(rnd_img_data)\n",
        "    if iterations % 100 == 0:\n",
        "       print('Iteration Number =',iterations)\n",
        "       print('Total loss =',t_loss)\n",
        "       print('Feature Maximisation =',l1)\n",
        "       print('Bounded regularisation =',l1_bnd)\n",
        "       print('Total Variation =',tv_loss)\n",
        "       print(nx)\n",
        "       #print('Total Variation Loss=',tv_loss)\n",
        "       display_image(rnd_img_data+127.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdw6cEqfc9aZ"
      },
      "source": [
        "## Finding M paramter (This is not requred unless setting up for different layers, networks etc)\n",
        "This paramter is determined by inputing an image (loaded via google drive in this case) to the network and finding the receptive value of the target neuron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7L54YaQZoW3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7bd14493-fd17-41ee-bb91-80ee5ca32493"
      },
      "source": [
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfrom google.colab import drive\\ndrive.mount('/content/gdrive')\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m286AAXyZqdQ"
      },
      "source": [
        "\"\"\"\n",
        "source_img_path = '/content/gdrive/My Drive/style_xfer_images/Red_Fox.jpg'\n",
        "#source_img_path = '/content/gdrive/My Drive/Photos/S1630013.JPG'\n",
        "\n",
        "\n",
        "img1_file = tf.io.read_file(source_img_path)\n",
        "img1 = tf.image.decode_jpeg(img1_file,channels=3)\n",
        "img1.set_shape([None, None, 3])\n",
        "img1_resized = tf.image.resize(img1, [H,W])\n",
        "img1_resized = tf.expand_dims(img1_resized, axis=0)\n",
        "\n",
        "output_vals=red_model(process_image(img1_resized))\n",
        "print(np.max(output_vals))\n",
        "\n",
        "M = np.max(output_vals) # Maximum value receptive component (neuron) from IMAGENET data\n",
        "Z = M*(rho**2)\n",
        "print(np.shape(output_vals))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}