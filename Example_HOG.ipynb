{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlSXC9sJvP+z+rKYyZ3E4p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyscan6003/etivities/blob/main/Example_HOG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyw9Civ7bEn2"
      },
      "source": [
        "# Histogram of Orientated Gradients (HOG) Feature Descriptor\n",
        "In this example we will explore using the Histogram of Orientated Gradients feature descriptor for pedestrian detection. We will use the built-in [HOG descriptor](https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html) from the Scikit Image python package for visualisation of the HOG descriptor.  We will then use the OpenCV built-in [HOG descriptor](https://docs.opencv.org/master/d5/d33/structcv_1_1HOGDescriptor.html) and pretrained SVM classifier to demonstrate full pedestrian detection at different scales in the test image.\n",
        "\n",
        "![HoG Visualisation ](https://github.com/tonyscan6003/etivities/blob/main/hog_image.jpg?raw=true\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTq6Zd85qDdA"
      },
      "source": [
        "##**HouseKeeping**:\n",
        "Import packages, define function to read image, read image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzIQ74CseLo"
      },
      "source": [
        "import numpy as np\n",
        "import urllib.request\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy import signal\n",
        "\n",
        "def url_to_image(url):\n",
        "\tresp = urllib.request.urlopen(url)\n",
        "\ttemp_image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "\ttemp_image = cv2.imdecode(temp_image, cv2.IMREAD_COLOR)\n",
        "\ttemp_image = cv2.cvtColor(temp_image, cv2.COLOR_BGR2RGB) # OpenCV defaults to BGR, but we need RGB here..\n",
        "\treturn temp_image\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-IMu28BP7sg"
      },
      "source": [
        "Lets Load an image from the web with pedestrians. (You can modify this to your own image. Note that you can adjust the scale of your image using scale factor if it is too large and causes the pedestrian detector later on to run slowly)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYK1USm0RwzG"
      },
      "source": [
        "# Set Image URL\n",
        "test_image_url = \"https://static01.nyt.com/images/2017/11/03/nyregion/00pedxing01/00pedxing01-superJumbo.jpg\"\n",
        "# Set Image Scale factor\n",
        "scale_factor = 4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxp1nukFsg20"
      },
      "source": [
        "# Read image\n",
        "test_image = url_to_image(test_image_url)\n",
        "\n",
        "# Resize image if required.\n",
        "test_size = np.shape(test_image)\n",
        "test_image = cv2.resize(test_image, dsize=(int(test_size[1]/scale_factor),int(test_size[0]/scale_factor)), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# Plot Test Image\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Test Image')\n",
        "plt.imshow(test_image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CUcikszR9rG"
      },
      "source": [
        "## HOG Descriptor Visualisation.\n",
        "The HOG Descriptor is based on gradients at each point in the image (or detector window).\n",
        "We will initially perform convolution of the input (greyscale) image with the simple x and y gradient filters, using the convolve2d command from scikit learn. We can plot these x,y gradients and obtain the magnitude and angle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS-dpikn0sc0"
      },
      "source": [
        "\n",
        "# Simple Gradient Filters\n",
        "grad_x_filter = [[0,0,0],[-1,0,1],[0,0,0]]\n",
        "grad_y_filter = [[0,-1,0],[0,0,0],[0,1,0]]\n",
        "\n",
        "# Greyscale image\n",
        "gray = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Convolve kernel with image and get magnitude and angle.\n",
        "conv_result_x = signal.convolve2d(gray, grad_x_filter, boundary='symm', mode='same')\n",
        "conv_result_y = signal.convolve2d(gray, grad_y_filter, boundary='symm', mode='same')\n",
        "conv_mag = np.sqrt(conv_result_x**2+conv_result_y**2)\n",
        "conv_theta = np.arctan(conv_result_y/conv_result_x)\n",
        "\n",
        "# Create Plots\n",
        "f, axarr = plt.subplots(1,4,figsize=(40,20))\n",
        "axarr[0].imshow(test_image)\n",
        "axarr[0].set_title('Test Image',fontsize =18)\n",
        "axarr[1].imshow(conv_result_x,'gray')\n",
        "axarr[1].set_title('Convolution with x-direction kernel',fontsize =18)\n",
        "axarr[2].imshow((conv_result_y),'gray')\n",
        "axarr[2].set_title('Convolution with y-direction kernel',fontsize =18)\n",
        "axarr[3].imshow(conv_mag,'jet')\n",
        "axarr[3].set_title('Magnitide of gradient',fontsize =18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAxdHnA5iVYo"
      },
      "source": [
        "We will now obtain and display the HOG Descriptor using [feature.hog](https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html) from the Scikit Image (skimage) package.\n",
        "\n",
        "It should be noted how the primary direction of the histograms aligns with the edges extracted with the gradient kernels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CheQ2zEPwJMX"
      },
      "source": [
        "from skimage import feature\n",
        "from skimage import exposure\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "H,hogImage = feature.hog(gray, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2),\n",
        "                         transform_sqrt=True, block_norm=\"L1\",visualize=True)\n",
        "hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255))\n",
        "hogImage = hogImage.astype(\"uint8\")\n",
        "cv2_imshow(hogImage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGCTKDLokiDJ"
      },
      "source": [
        "##Pedestrian Detection with HOG\n",
        "In the code cells below we will use the [OpenCV HOG Descriptor](https://docs.opencv.org/master/d5/d33/structcv_1_1HOGDescriptor.html) (HOGDescriptor) and pretrained SVM Classifier (hog.setSVMDectector) to perform pedestrian detection. The method (hog.detectMultiScale) will perform detection with a sliding window over an image pyramid, we can control the number steps the sliding window moves over the image (`win_stride`) and the degree of scaling in the pyramid (`pyramid_sf`)\n",
        "\n",
        "You should adjust these parameters to see the effect of more/less levels in the pyramid and the spacing used for the sliding detector. In particular you may want to change these parameters to suit a different test image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj4Mx9XmtDUx"
      },
      "source": [
        "# Set up Descriptor and SVM Classifier\n",
        "hog = cv2.HOGDescriptor()\n",
        "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehAPh6jZqZ46"
      },
      "source": [
        "win_stride = 4     # Sliding window stride\n",
        "pyramid_sf = 1.2  # pyramid scaling factor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eFEiwwwqff3"
      },
      "source": [
        "The method hog.detectMulitscale generates its own pyramid and we don't need to create one in advance. As a short aside we will visualise an Image pyramid (We can use some code from scikit Image to visulaise the [image pyramid](https://scikit-image.org/docs/dev/auto_examples/transform/plot_pyramid.html) for our image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErpJq4I9qi_Y"
      },
      "source": [
        "from skimage.transform import pyramid_gaussian\n",
        "\n",
        "# image size\n",
        "rows, cols, dim = test_image.shape\n",
        "\n",
        "# Generate pyramid\n",
        "pyramid = tuple(pyramid_gaussian(test_image, downscale=pyramid_sf, multichannel=True))\n",
        "\n",
        "#plot first 5 levels of pyramid\n",
        "f, axarr = plt.subplots(1,5,figsize=(20,40))\n",
        "for i in range(5):\n",
        "    p = pyramid[i]\n",
        "    composite_image = np.zeros((rows, cols, 3), dtype=np.double)\n",
        "    n_rows, n_cols = p.shape[:2]\n",
        "    composite_image[0:n_rows, 0:n_cols] = p\n",
        "    axarr[i].imshow(composite_image)\n",
        "    axarr[i].title.set_text('Pyramid level '+str(i))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmsahMxGsHww"
      },
      "source": [
        "Run HOG detection over image pyramid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbXZ3OZUtVW0"
      },
      "source": [
        "\n",
        "# run detection, using a spatial stride of 4 pixels (horizontal and verticle), a scale factor of 1.2,\n",
        "(foundBoundingBoxes, weights) = hog.detectMultiScale(test_image, winStride=(win_stride, win_stride),\n",
        "                                                     padding=(2*win_stride, 2*win_stride), scale=pyramid_sf, finalThreshold=0)\n",
        "\n",
        "# Draw bounding boxes around pedestrian detections.\n",
        "redColor = (255, 0, 0)\n",
        "lineThickness = 1\n",
        "imgWithRawBboxes = test_image.copy()\n",
        "for (hx, hy, hw, hh) in foundBoundingBoxes:\n",
        "        cv2.rectangle(imgWithRawBboxes, (hx, hy), (hx + hw, hy + hh), redColor, lineThickness)\n",
        "\n",
        "# Plot Image with bounding boxes shown.\n",
        "plt.figure(figsize=(14, 10), dpi=80)\n",
        "plt.title('HoG based Pedestrian Detection')\n",
        "plt.imshow(imgWithRawBboxes, aspect='auto')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrhAay4yoXCG"
      },
      "source": [
        "Finally we will obsevere the effect of applying Non-Maximum Suppression to remove excess detections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tw4tzAXvdhh"
      },
      "source": [
        "from imutils.object_detection import non_max_suppression\n",
        "\n",
        "# convert our bounding boxes from format (x1, y1, w, h) to (x1, y1, x2, y2)\n",
        "rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in foundBoundingBoxes])\n",
        "\n",
        "# run non-max suppression on these based on an overlay op 65%\n",
        "nmsBoundingBoxes = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
        "\n",
        "print (\"Before suppression, we had {} bounding boxes, after suppression we have {}\".format(len(rects), len(nmsBoundingBoxes)))\n",
        "\n",
        "greenColor = (0, 255, 0)\n",
        "lineThickness = 1\n",
        "# draw the final bounding boxes\n",
        "for (xA, yA, xB, yB) in nmsBoundingBoxes:\n",
        "    cv2.rectangle(test_image, (xA, yA), (xB, yB), greenColor, lineThickness)\n",
        "\n",
        "# Plot figures\n",
        "plt.figure(figsize=(14, 10), dpi=80)\n",
        "plt.imshow(test_image, aspect='auto')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}